{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Para realizar este tutorial abrimos un cuaderno de Jupyter <https://jupyter.org/> en nuestro servidor local o utilizamos Colaboratory: <https://colab.research.google.com>.\n",
    "- Los pasos son los siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Actualizamos nuestro sistema operativo e instalamos Java y Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDdBqTQ_FMQb",
    "outputId": "e02257db-39a3-4b5e-8afe-604c02938e0c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "apt update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9mWowO8FUGQ",
    "outputId": "c3961131-abf4-4985-91e0-b809a804f212",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "apt upgrade --fix-broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3AeCQhKCz2Y",
    "outputId": "65d17a40-e8aa-4cce-bbdd-b03cd6a76c7b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "apt install default-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PycxFtjTjVdS",
    "outputId": "ab8f41cf-41f1-4252-abab-5c3dfb095822",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "apt autoremove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_0veBkAH8wA",
    "outputId": "996ddf50-b295-413d-a35e-1d759926f022",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "apt install default-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yf33QuaIf8m",
    "outputId": "050a7185-ed63-4d84-be0e-d947a0c95d1b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "apt install scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYrDrt-aIE48",
    "outputId": "bd7dc971-749b-484d-b0d5-5de0c08b06ba",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.13\" 2021-10-19\n",
      "OpenJDK Runtime Environment (build 11.0.13+8-Ubuntu-0ubuntu1.20.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.13+8-Ubuntu-0ubuntu1.20.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gpx68SHIU5i",
    "outputId": "f01586e0-ac40-4a0b-aad7-95366ab26db1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javac 11.0.13\n"
     ]
    }
   ],
   "source": [
    "javac -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjD-S4VTIsCq",
    "outputId": "148e54d2-746c-494f-cc79-9e30368c558a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scala -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG13NQn3Lq0O",
    "outputId": "94c894f1-2bb3-4f83-f61a-5e4cacaf23a2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64/bin/java\n"
     ]
    }
   ],
   "source": [
    "readlink -f $(which java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Descargamos e instalamos Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7TsaEKmhDLws",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTuf_ah2Qs4h",
    "outputId": "543ca8ad-27b9-48ef-c25b-88df4cffd7be",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-23 15:19:24--  https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz.asc\n",
      "Resolviendo downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f9:3a:2c57::2, ...\n",
      "Conectando con downloads.apache.org (downloads.apache.org)[135.181.214.104]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 833 [text/plain]\n",
      "Guardando como: “spark-3.2.1-bin-hadoop3.2.tgz.asc.1”\n",
      "\n",
      "spark-3.2.1-bin-had 100%[===================>]     833  --.-KB/s    en 0s      \n",
      "\n",
      "2022-02-23 15:19:25 (119 MB/s) - “spark-3.2.1-bin-hadoop3.2.tgz.asc.1” guardado [833/833]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz.asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvQg4-JYRZMY",
    "outputId": "7c691e49-6515-4e0f-c813-a0a6780052f7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wget https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz.sha512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_00jpPcCRdN4",
    "outputId": "d8b6a7fb-f817-4739-ce26-bac002b2e760",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-23 15:19:26--  https://downloads.apache.org/spark/KEYS\n",
      "Resolviendo downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f9:3a:2c57::2, ...\n",
      "Conectando con downloads.apache.org (downloads.apache.org)[135.181.214.104]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 101227 (99K)\n",
      "Guardando como: “KEYS.1”\n",
      "\n",
      "KEYS.1              100%[===================>]  98,85K   121KB/s    en 0,8s    \n",
      "\n",
      "2022-02-23 15:19:28 (121 KB/s) - “KEYS.1” guardado [101227/101227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget https://downloads.apache.org/spark/KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqdEMlyyPKH-",
    "outputId": "181aacab-5636-4deb-b7c1-b8c658ef504e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gpg --import KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnTriHwDPwgL",
    "outputId": "d7265674-6c21-4050-d38c-252451e8ae9e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpg: Firmado el jue 20 ene 2022 21:10:50 CET\n",
      "gpg:                usando RSA clave CEA888BDB32D983C7F094564AC01E6E9139F610C\n",
      "gpg: Firma correcta de \"Huaxin Gao (CODE SIGNING KEY) <huaxin.gao11@gmail.com>\" [desconocido]\n",
      "gpg: ATENCIÓN: ¡Esta clave no está certificada por una firma de confianza!\n",
      "gpg:          No hay indicios de que la firma pertenezca al propietario.\n",
      "Huellas dactilares de la clave primaria: CEA8 88BD B32D 983C 7F09  4564 AC01 E6E9 139F 610C\n"
     ]
    }
   ],
   "source": [
    "gpg --verify spark-3.2.1-bin-hadoop3.2.tgz.asc  spark-3.2.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y03wmIQqJrvF",
    "outputId": "bea48834-9284-4275-e025-e9ba1670fb58",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jllky98qiNpI",
    "outputId": "e05ec17a-0d56-4e77-e3cd-7d4eb3e201f3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tar xvf spark-3.2.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Instalamos e configuramos la biblioteca de Python *findspark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xfDRMPWyiUsD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/usuario/gitRepos/spark/sparkFiles\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jhXEW1tvispk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"spark-3.2.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizamos la biblioteca *findspark* para crear un *dataframe* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oENdTCb3ixeX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wlFyD66jXgt",
    "outputId": "dd6e1df8-e1bb-4e90-a6eb-cf9a3faa2076",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([{\"Hola\": \"mundo\"} for x in range(1000)])\n",
    "df.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer, transformar y selecionar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Importamos la biblioteca *pyspark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvPgB5A8jv99",
    "outputId": "c699545c-84d5-40eb-c2b0-80fc4c44b11a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformar significa escalar, convertir o modificar las características de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXYpxt_2n9yq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-  Uno de los primeros pasos en NLP (Natural Language Processing) es convertir el texto en tokens o palabras *tokenizadas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnysDT74pREI",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul-miJo9pjdB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "oraciones_df = spark.createDataFrame([\n",
    "    (1, \"Introducción a sparkMlib\"),\n",
    "    (2, \"Mlib incluye bibliotecas para clasificación y regresión\"),\n",
    "    (3, \"También incluye soporte a datapipe lines\"),\n",
    "    \n",
    "], [\"id\", \"oraciones\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iq-nCXgGplzG",
    "outputId": "3067cd64-08f6-4086-8a0c-a76ce08887ce",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "oraciones_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cGzQwCzoj4f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Para reflejar la importancia de una palabra en un texto utilizamos *Term frequency-inverse document frequency (TF-IDF)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya99PPU2pvjr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent_token = Tokenizer(inputCol = \"oraciones\", outputCol = \"palabras\")\n",
    "sent_tokenized_df = sent_token.transform(oraciones_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVleGpkLpx9X",
    "outputId": "97e0efcd-c354-4823-adba-66b65d120bb3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent_tokenized_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sIt1LyzqJ4g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol = \"palabras\", outputCol = \"rawfeatures\", numFeatures = 20)\n",
    "sent_fhTF_df = hashingTF.transform(sent_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFvef68oqLhT",
    "outputId": "6d783ea9-e3be-4fa1-bac1-f17d9bf8ba38",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent_fhTF_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXr0kWG7piuK",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol = \"rawfeatures\", outputCol = \"idffeatures\")\n",
    "idfModel = idf.fit(sent_fhTF_df)\n",
    "tfidf_df = idfModel.transform(sent_fhTF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hewPehYypu1B",
    "outputId": "ee2730ed-b097-4d80-bfad-a71ae6942a89",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fuente: <https://spark.apache.org/docs/3.0.1/ml-features.html#tokenizer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWgotn_amdIR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Utilizamos la clase StandardScaler para estandarizar datos en ML.\n",
    "- Los escala entre -1 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-tfovXtngnd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import  StandardScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJnERy6Onlud",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
    "    (2, Vectors.dense([20.0,40000.0,2.0]),),\n",
    "    (3, Vectors.dense([30.0,50000.0,3.0]),),\n",
    "    \n",
    "],[\"id\", \"features\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOdvaKRanp_A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd=True, withMean=True)\n",
    "stmodel = features_stand_scaler.fit(features_df)\n",
    "stand_sfeatures_df = stmodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpxHUrMjntAk",
    "outputId": "4a539adb-b51b-47c6-aa52-8d82c1938c1b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stand_sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLC2HfkLoFSk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Fuente: MA Raza, Ph.D. <https://towardsdatascience.com/machine-learning-with-spark-f1dbc1363986>.\n",
    "- Spark docs: <https://spark.apache.org/docs/3.0.1/ml-features.html#standardscaler>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68UujMyXlrwq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Utilizamos la clase MinMaxScaler en ML para normalizar datos numéricos. \n",
    "- Escala los datos entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl99b-vzmDBo",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeFaVnNXmSPr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
    "    (2, Vectors.dense([20.0,40000.0,2.0]),),\n",
    "    (3, Vectors.dense([30.0,50000.0,3.0]),),\n",
    "    \n",
    "],[\"id\", \"features\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzU2P5t8mWVI",
    "outputId": "8d136442-6f0c-4472-9190-10aa6099f2f3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erCXuvy9l8mR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Aplicamos la transformación de la biblioteca MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15c-PBUGmddd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")\n",
    "smodel = features_scaler.fit(features_df)\n",
    "sfeatures_df = smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZMznmmemhOL",
    "outputId": "5c0a3cbc-ddf0-4b43-f0d6-ef35ebbf351e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fuente: <https://spark.apache.org/docs/3.0.1/ml-features.html#minmaxscaler>\n",
    "- El ejemplo completo se encuentra en *examples/src/main/python/ml/min_max_scaler_example.py* dentro del repositorio de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47hqYeu0nLkE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La clase Bucketizer transforma los datos en varias frecuencias o *buckets*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fciz7N9korpu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import  Bucketizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx5QlSrjotdO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "splits = [-float(\"inf\"), -10, 0.0, 10, float(\"inf\")]\n",
    "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
    "b_df = spark.createDataFrame(b_data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXvNhi75oxR-",
    "outputId": "c6f988cd-7a9d-4cdd-cf13-f5f9c41494e7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2XBYO5VpFbl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol= \"features\", outputCol=\"bfeatures\")\n",
    "bucketed_df = bucketizer.transform(b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WACP9Tk7pONx",
    "outputId": "53910a7a-b4b7-43a8-d709-08cc9798d668",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: <https://spark.apache.org/docs/3.0.1/ml-features.html#bucketizer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VuXzsGAqNMK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Para agrupar datos en un razonable grupo de frecuencias se puede utilizar como técnica el llamado *clustering*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Qvm1RL1qkGk",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4qlu12jOql5G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Carga los datos\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i85UUJyXqzLW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Entrena el modelo k-means\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dfpoAFtq2Mf",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Hace predicciones\n",
    "predictions = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFggRBa0q7ay",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evalua utilizando Silhouette score\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUw_YOUErEtn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqm-ChCXrI8j",
    "outputId": "f2b74504-533d-4d4f-e443-7a0574aedfef",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJjiwpO4rPyi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Fuente: <https://spark.apache.org/docs/3.0.1/ml-clustering.html#k-means>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2HAxpH4rkUG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Otro algoritmo de *clustering* implementado en MLlib es el llamado *Bisecting K-Means*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JZNUkqCrSeo",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYJH-LC0rawQ",
    "outputId": "11b318da-2291-4f5e-dda3-7cd1d5271a61",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo bisecting k-means\n",
    "bkm = BisectingKMeans().setK(2).setSeed(1)\n",
    "model = bkm.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfbzD5ivrsVj",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Hace predicciones\n",
    "predictions = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqrwS8tEr_Tt",
    "outputId": "24ab53f1-d844-4bbf-ac47-9669c0ed36b1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Evalua utilizando Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra los resultados\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: <https://spark.apache.org/docs/3.0.1/ml-clustering.html#bisecting-k-means>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IGwtaPAr2Tg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión utilizando PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-xwO5UYwC7m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- En este ejemplo se realizara una regresión logística binomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_2w2fJrtECf",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaCDI2RUtHT6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Se cargan los datos\n",
    "training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2h1130SWtLEr",
    "outputId": "09a868c1-19d2-411a-bffb-905ace0edbe7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjcWjDanwkYv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ajusta el modelp\n",
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG6n0VjRx6nm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imprime los coeficientes y el intercepto para la regresión logística\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mM81h7ULx9lq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Se puede usar también la familia multinomial para clasificación binaria\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bukez5phyAJN",
    "outputId": "41cf4b37-3d4b-4ac5-9a73-d73440a7fdcd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Ajusta el modelo\n",
    "mlrModel = mlr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9jaP9lDyRME",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imprima los coeficientes e intersecciones para la regresión logística con familia multinomial\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVsoVKAEw89I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Fuente: <https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#binomial-logistic-regression>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PAZeZpJx2SS",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clasificación Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_EPDhl1y3gI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmKYf2KGyGxK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Creamos los *splits* de entrenamiento y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zvJ5yO-y6j8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.format(\"libsvm\") \\\n",
    "    .load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kohz0jaIyQts",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Aplicamos la clasificación Naive bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUE4PbEAy842",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QuFL1wRy_ep",
    "outputId": "063f94ba-fee7-4605-aee4-679fc405191a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esIOB_QZyZAJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Evaluamos el clasificador entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpJZwP7DzKkR",
    "outputId": "46ff8994-207b-4286-d630-b0e58bf58188",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fuente: <https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#naive-bayes>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLTRxLahy1O1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasificación con árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFt0y8N00Dzu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7R98d85z7e2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cargar los datos en formato LIBSVM como un DataFrame\n",
    "data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels agregan metadata a la columna label\n",
    "# Ajuste en todo el conjunto de datos para incluir todas las etiquetas en el índice\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "# Identifica automáticamente características categóricas y las indexa\n",
    "# Especifica maxCategories para que las entidades con > 4 valores distintos se traten como continuas\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjmd3wfy0KFt",
    "outputId": "def8a603-0bba-48a2-cae9-e631b1023a93",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Divide los datos en conjuntos de entrenamiento y prueba (30% retenido para prueba)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Entrena el modelo DecisionTree\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Encadena índices y árboles en tuna Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Entrena el modelo y ejecuta los indexadores\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Hace predicciones\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Selecciona filas de jemplo para mostrar\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Selecciona (predicción, etiqueta verdadera) y calcula el error de prueba\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# Resumen\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMTe4EcozWvM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fuente: <https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#decision-tree-classifier>\n",
    "- Se pueden encontrar otros algoritmos de clasificación de la biblioteca Spark MLLib en:     \n",
    "<https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#classification>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "spark_ML_tutorial2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
