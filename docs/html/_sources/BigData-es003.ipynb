{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149cb3a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data tema 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4ea0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Procesos en Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c851634",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Para proyectos de Big Data los procesos básicamente se pueden dividir en cuatro fases: \n",
    "    - Ingestión de datos (*Data Ingestion*).\n",
    "    - Almacenamiento de datos (*Data Storage*).\n",
    "    - Procesamiento, análisis y consulta de datos (*Data Processing / Data Query*).\n",
    "    - Visalización de datos (*Data Visualization*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425d9e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Procesamiento, análisis y consulta de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122cb08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Los datos recolectados en la fase anterior serán procesados en este paso.\n",
    "- Aquí, el sistema de procesamiento de canalización de datos enruta los datos a un destino diferente, clasifica el flujo de datos y es el primer punto donde puede tener lugar el análisis.\n",
    "- Esta es la capa donde las consultas (*queries*) y el proceso analítico activo se ejecutan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10130ce6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Para ello, los analistas emplean diferentes herramientas y estrategias como, por ejemplo:\n",
    "    - Modelado estadístico\n",
    "    - Algoritmos\n",
    "    - Inteligencia artificial (AI)\n",
    "    - Minería de datos\n",
    "    - Aprendizaje automático (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f69e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Las consultas interactivas para el procesamiento de datos son necesarias y es una zona tradicionalmente dominada por desarrolladores expertos en SQL.\n",
    "- Con Hadoop, la ingesta de datos, el almacenamiento, el proceso y el análisis se volvieron fáciles de trabajar cuando se cuenta con una gran cantidad de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742eb8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Para análisis fuera de línea, se utiliza un sistema de procesamiento por lotes simple. \n",
    "- Apache Sqoop es la aplicación que se encarga de esto.\n",
    "- Transfiere eficientemente datos estructurados entre Apache Hadoop y las bases de datos relacionales.\n",
    "- Spark por otro lado, es utilizado mayoritariamente para el análisis y procesamiento de datos en tiempo real.\n",
    "- Otra herramienta pero menos utilizada es Apache Storm.\n",
    "    - Extraído de: <https://sqoop.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb84c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- KNIME hace que la comprensión de datos y el diseño de flujos de trabajo de ciencia de datos y componentes reutilizables sean accesibles para todos.\n",
    "    - <https://www.knime.com/>.\n",
    "- Apache Mahout es un framework distribuido de álgebra linear y Scala DSL matemáticamente expresivo.  \n",
    "    - <https://mahout.apache.org/>.\n",
    "- Weka 3: Machine Learning Software en Java para hacer análisis simple.\n",
    "    - <https://www.cs.waikato.ac.nz/ml/weka/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2b8d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Materialize es una base de datos útil para análisis en tiempo real que ofrece actualizaciones de vista incrementales.\n",
    "    - Extraído de: <https://materialize.com/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0f15c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Scoop (Attic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833c8b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Transfiere eficientemente datos estructurados entre Apache Hadoop y las bases de datos relacionales.\n",
    "    - Extraído de: <https://sqoop.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299983b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Se puede usar Sqoop para importar datos desde un sistema de administración de bases de datos relacionales (RDBMS) como MySQL u Oracle o un mainframe al sistema de archivos distribuidos de Hadoop (HDFS), transformar los datos en Hadoop MapReduce y luego exportar los datos nuevamente a un RDBMS.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d067b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Apache Sqoop también puede ser utilizado para extraer datos de Hadoop y exportarlos a almacenes de datos estructurados externos.\n",
    "- Apache Sqoop trabaja con bases de datos relacionales como Teradata, Netezza, Oracle, MySQL, Postgres, y HSQLDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3f045",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sqoop automatiza la mayor parte de este proceso, basándose en la base de datos para describir el esquema de los datos que se importarán.\n",
    "- Sqoop utiliza MapReduce para importar y exportar los datos, lo cual proporciona procesamiento en paralelo y tolerancia a fallos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9918909",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d0928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Framework de procesamiento paralelo y de código abierto para ejecutar aplicaciones de análisis de datos a gran escala en sistemas agrupados.\n",
    "    - Extraído de: <https://spark.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af111679",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- También es compatible con un amplio conjunto de herramientas de alto nivel, que incluyen:\n",
    "    - Spark SQL para SQL y procesamiento de datos estructurados, \n",
    "    - MLlib para aprendizaje automático, \n",
    "    - GraphX para procesamiento de gráficos, y \n",
    "    - Transmisión estructurada para procesamiento incremental y de streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b034ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Spark SQL incluye un optimizador basado en costes, almacenamiento en columnas y generación de código para agilizar las consultas.\n",
    "- Al mismo tiempo, se escala a miles de nodos y consultas de varias horas mediante el motor Spark, que proporciona tolerancia completa a fallas en la mitad de la consulta. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69baf8a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Se utiliza para realizar trabajos informáticos con grandes cargas de datos junto a Apache Kafka. \n",
    "- Fue desarrollado en la University of California, Berkeley.\n",
    "- Con Spark ejecutándose en Apache Hadoop YARN, los desarrolladores pueden crear aplicaciones para explotar el poder de Spark, obtener información y enriquecer sus cargas de trabajo de ciencia de datos dentro de un único conjunto de datos compartidos en Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb785f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Storm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01001816",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Es un sistema distribuido open source para procesar datos en tiempo real durante la ingesta de datos. \n",
    "- Es escalable, tiene tolerancia a fallos, y es fácil de configurar y operar.\n",
    "- Facilita el procesamiento confiable de flujos ilimitados de datos, haciendo para el procesamiento en tiempo real lo que Hadoop hizo para el procesamiento por lotes.\n",
    "    - Extraído de: <https://storm.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c3bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- No existe ningún truco que convierta a Hadoop en un sistema en tiempo real.\n",
    "- El procesamiento de datos en tiempo real tiene un conjunto de requisitos fundamentalmente diferente al procesamiento por lotes.\n",
    "- Apache Storm agrega a Hadoop esta funcionalidad de manera sencilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89507b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Casos de uso:\n",
    "    - Análisis en tiempo real\n",
    "    - Aprendizaje automático\n",
    "    - Monitoreo continuo de operaciones\n",
    "    - RPC distribuido, ETL, y más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c26e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storm vs. Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90c48b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Situation | Spark | Storm |\n",
    "|------- | ---------------- | -------- | \t\t\t\n",
    "| Stream processing | Batch processing | Micro-batch processing |\n",
    "| Latency | Latency of a few seconds | Latency of milliseconds |\n",
    "| Multi-language support | Lesser language support | Multiple language support |\n",
    "| Languages | Java – Scala | Java – Scala – Clojure |\n",
    "| Stream sources | HDFS | Spout |\n",
    "| Resource management | Yarn, Mesos | Yarn, Mesos |\n",
    "| Provisioning | Basic using Ganglia | Apache Ambari |\n",
    "| Messaging | Netty, Akka | ZeroMQ, Netty |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98ac4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Impala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da949bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Impala eleva el nivel de rendimiento de las consultas SQL en Apache Hadoop al mismo tiempo que conserva una experiencia de usuario familiar.\n",
    "- Con Impala, se puede consultar datos, ya sea que estén almacenados en HDFS o Apache HBase, incluidas las funciones SELECT, JOIN y agregadas, en tiempo real. \n",
    "    - Extraído de: <https://impala.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fada5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Impala utiliza los mismos metadatos, sintaxis SQL (Hive SQL), controlador ODBC e interfaz de usuario (Hue Beeswax) que Apache Hive, lo que proporciona una plataforma familiar y unificada para consultas en tiempo real o por lotes.\n",
    "- Los usuarios de Hive pueden utilizar Impala con algunos pocos ajustes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43451a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Kudu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1939bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Apache Kudu es un motor de almacenamiento columnar open source para datos estructurados.\n",
    "- Está diseñado y optimizado para análisis de big data en datos que cambian rápidamente o para un rendimiento rápido en consultas analíticas - OLAP. \n",
    "- Es distribuido, permite varios tipos de partición de datos y carga compartida en varios servidores. \n",
    "- Es parte del ecosistema Hadoop y se integra con frameworks de procesamiento de datos como Spark, Impala y MapReduce.\n",
    "    - <https://kudu.apache.org/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3fe16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd3258",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Apache Hive es una infraestructura de almacenamiento de datos construida sobre Apache Hadoop para proporcionar resúmenes de datos, consultas ad-hoc y análisis de grandes conjuntos de datos.\n",
    "- Los analistas de datos usan Hive para consultar, resumir, explorar y analizar esos datos, y luego convertirlos en información empresarial procesable. \n",
    "    - Fuente: <https://hive.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7ac6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- El software de almacenamiento de datos Apache Hive facilita la lectura, escritura y administración de grandes conjuntos de datos que residen en almacenamiento distribuido mediante SQL.\n",
    "- La estructura se puede proyectar sobre los datos que ya están almacenados.\n",
    "- Se proporciona una herramienta de línea de comandos y un controlador JDBC para conectar a los usuarios a Hive.\n",
    "- Proporciona un mecanismo para la estructura del proyecto de ingesta de datos en los datos de Hadoop y para consultar esos datos mediante con un lenguaje tipo SQL llamado HiveQL (HQL). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6144c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Drill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba36413",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Es un motor SQL para Hadoop, NoSQL y almacenamiento en la nube.\n",
    "- Soporta variadas bases de datos NoSQL y sistemas de ficheros:\n",
    "- HBase, MongoDB, HDFS, Amazon S3, Azure Blob Storage, Google Cloud Storage, NAS y ficheros locales.\n",
    "- Una única consulta puede obtener datos de múltiples bases de datos.\n",
    "    - Fuente: <https://drill.apache.org/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38a67e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670af75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Motor SQL desarrollado por Facebook para análisis ad-hoc e informes rápidos.\n",
    "- Es un motor de consulta SQL distribuido de código abierto que ejecuta consultas analíticas interactivas en fuentes de datos de todos los tamaños, desde gigabytes hasta petabytes.\n",
    "    - Fuente: <https://prestodb.io/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0bb7fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Facebook usa Presto para consultas interactivas en varios almacenes de datos internos, incluido su almacén de datos de 300 PB. \n",
    "- Más de 1000 empleados de Facebook usan Presto diariamente para ejecutar más de 30000 consultas que, en total, escanean más de un petabyte por día."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b33f21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fff0e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Trino es un motor de consulta SQL distribuido diseñado para consultar grandes conjuntos de datos distribuidos en una o más fuentes de datos heterogéneas.\n",
    "- Son los fundadores del proyecto Presto que tuvieron que cambiarle el nombre por cuestiones legales.\n",
    "    - Fuente: <https://trino.io/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b977f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Hadoop YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08cee6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Tecnología de gestión de clústeres en Hadoop de segunda generación.\n",
    "- La idea de YARN es dividir las funcionalidades de gestión de recursos y programación/supervisión de trabajos en servicios separados: un *ResourceManager* (RM) global y un *ApplicationMaster* (AM) por aplicación.\n",
    "- Una aplicación es un solo trabajo o un DAG de trabajos.\n",
    "- El *ResourceManager* y el *NodeManager* forman el framework de cálculo de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df801866",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fuente: <https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5a5f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- El *ResourceManager* arbitra los recursos entre todas las aplicaciones del sistema.\n",
    "- El *NodeManager* es el agente del framework por máquina que es responsable de los contenedores, monitorea el uso de sus recursos (cpu, memoria, disco, red) e informa al *ResourceManager/Scheduler*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da854ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La aplicación *ApplicationMaster* es en efecto una biblioteca específica del framework y tiene la tarea de negociar recursos del *ResourceManager* y trabajar con el/los *NodeManager(s)* para ejecutar y monitorear las tareas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8243b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Otras herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a812a59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Vertica es un almacén de datos (*data warehouse*) de análisis unificado.\n",
    "- Ha sido diseñado para ofrecer velocidad, escalabilidad y aprendizaje automático integrado para cargas de trabajo analíticamente intensivas.\n",
    "    - Extraído de <https://www.vertica.com/landing-page/start-your-free-trial-today/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0a77d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hue es un asistente open source de SQL para bases de datos y *data warehouses*.\n",
    "    - Fuente: <https://gethue.com/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f5646",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perfilado de datos y linaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739dddfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Perfilado de datos y linaje (*data profiling and lineage*) son técnicas que permiten identificar la calidad de los datos y su ciclo de vida durante las varias fases que percorren. \n",
    "- Es importante capturar los metadatos en cada paso del proceso para que puedan ser utilizados posteriormente para verificación y personalización.\n",
    "- Algunas aplicacione disponibles: Talend, Hive, Pig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b1d1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Talend Data Fabric es un conjunto de aplicaciones nativas de la nube que lidera la industria en integración y gestión de datos:\n",
    "    - Identifica elementos de datos.\n",
    "    - Realiza un seguimiento hasta el origen de los datos.\n",
    "    - Combina fuentes de datos y enlaces a los mismos.\n",
    "    - Crea un mapa para cada sistema y un mapa maestro de la imagen completa. \n",
    "    - Extraído de: <https://www.talend.com/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e3b39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- OpenLineage es un framework open source para la recopilación y el análisis del linaje de datos\n",
    "- Permite una recopilación consistente de metadatos de linaje, creando una comprensión más profunda de cómo se producen y utilizan los datos. \n",
    "    - Extraído de: <https://openlineage.io/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeea3bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calidad de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479d622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La ingesta de datos se considera de alta calidad si cumple con las necesidades comerciales y satisface el uso previsto, de modo que sea útil para tomar decisiones de negocio con éxito.\n",
    "- Por lo tanto, es un paso importante entender la dimensión de mayor interés e implementar métodos para lograrla.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9501b88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dd791",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La limpieza de datos (*data cleansing*) significa implementar varias soluciones para corregir datos incorrectos o corruptos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98ca29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prevención y pérdida de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9f5dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Deben existir políticas para asegurarse de que se resuelvan las lagunas en la pérdida de datos.\n",
    "- La identificación de dicha pérdida de datos necesita un control cuidadoso y procesos de evaluación de la calidad en el flujo del proceso de ingesta de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42df12c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ce4da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La fase de visualización o presentación es donde los usuarios pueden sentir el VALOR de los DATOS. \n",
    "- La visualización de hallazgos ayuda a tomar mejores decisiones de negocios.\n",
    "- Aquí se generan informes por tipo de audiencia (comerciales, marketing, estrategia, técnicos, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ff58f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Si bien está diseñado para manejar y almacenar grandes volúmenes de datos, Hadoop y otras herramientas no tienen disposiciones integradas para la visualización de datos y la distribución de información, lo que no permite que los usuarios finales del negocio puedan consumir fácilmente esos datos en la canalización de ingesta de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f38a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Los paneles personalizados son útiles para crear vistas generales únicas que presentan los datos de manera diferente.\n",
    "- Puede mostrar la información de la aplicación web y móvil, la información del servidor, los datos de métricas personalizadas y los datos de métricas de complementos, todo en un único tablero personalizado.\n",
    "- Los paneles en tiempo real guardan, comparten y comunican información.\n",
    "- Ayuda a los usuarios a generar preguntas al revelar la profundidad, el rango y el contenido de los almacenes de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f392b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Los paneles de visualización de datos siempre cambian a medida que llegan nuevos datos.\n",
    "- Los tableros pueden contener múltiples visualizaciones de múltiples conexiones una al lado de la otra.\n",
    "- Se pueden crear, editar, filtrar y eliminar tableros rápidamente y moverlos y cambiar su tamaño y luego compartirlos o integrarlos en su aplicación web.\n",
    "- Se puede exportar un tablero como una imagen o utilizando una configuración de archivo tipo JSON. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0b82a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d7c49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Elasticsearch es un motor de análisis y búsqueda de código abierto distribuido para todo tipo de datos, incluidos textuales, numéricos, geoespaciales, estructurados y no estructurados.\n",
    "- Elasticsearch se basa en Apache Lucene y fue lanzado por primera vez en 2010 por Elasticsearch N.V. (ahora conocido como Elastic). \n",
    "    - Extraído de: <https://www.elastic.co/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f374b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Lucene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce9352",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Lucene Core es una biblioteca de Java que proporciona potentes funciones de indexación y búsqueda, así como funciones de corrección ortográfica, resaltado de aciertos y análisis/tokenización avanzados.\n",
    "- El subproyecto PyLucene proporciona enlaces de Python para Lucene Core. \n",
    "    - Extraído de: <https://lucene.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8da04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Solr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500dc17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Solr es un servidor de búsqueda de alto rendimiento creado con Lucene Core.\n",
    "- Solr es altamente escalable y proporciona indexación, búsqueda y análisis distribuidos totalmente tolerantes a fallas.\n",
    "- Expone las características de Lucene a través de interfaces JSON/HTTP fáciles de usar o clientes nativos para Java y otros lenguajes. \n",
    "    - Extraído de: <https://lucene.apache.org/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce41c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elastic Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cea522",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Conocido por sus API REST simples, naturaleza distribuida, velocidad y escalabilidad, Elasticsearch es el componente central de Elastic Stack, un conjunto de herramientas de código abierto para la ingesta, el enriquecimiento, el almacenamiento, el análisis y la visualización de datos.\n",
    "- El ELK Stack se compone de las aplicaciones Elasticsearch, Logstash, Kibana, y Beats.\n",
    "- Actualmente se lo conoce como Elastic Stack, nombre que le permite agregar nuevas funcionalidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02961dd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Beats 8.0\n",
    "- Elasticsearch 8.0\n",
    "- Kibana 8.0\n",
    "- Logstash 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131ff95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Extraído de: <https://www.elastic.co/what-is/elasticsearch>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542228d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Casos de uso:\n",
    "    - Búsqueda de aplicaciones\n",
    "    - Búsqueda de sitio web\n",
    "    - Búsqueda Empresarial\n",
    "    - Logging y analíticas de log\n",
    "    - Métricas de infraestructura y monitoreo de contenedores\n",
    "    - Monitoreo de rendimiento de aplicaciones\n",
    "    - Análisis y visualización de datos geoespaciales\n",
    "    - Analítica de Seguridad\n",
    "    - Analítica de Negocios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaef331",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909590f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Kibana es una herramienta de visualización y gestión de datos para Elasticsearch que brinda histogramas en tiempo real, gráficos circulares y mapas. \n",
    "- Kibana también incluye aplicaciones avanzadas, como Canvas, que permite a los usuarios crear infografías dinámicas personalizadas con base en sus datos, y Elastic Maps para visualizar los datos geoespaciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45d556",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Salesforce, Google y Microsoft  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd69b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Otras herramientas de modelización y visualización de datos son las proporcionadas por Salesforce, Google y Microsoft.\n",
    "- Salesforce adquirió Tableau y google hizo lo mismo con Looker en 2019.\n",
    "- La reciente integración entre estos productos le permitirán al usuario modelar datos con LookML y usar Tableau, o Looker para explorar ese modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93d1f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Tableau es una plataforma de análisis de datos que puede ser implementada en la nube, localmente o integrada de forma nativa con Salesforce CRM. \n",
    "- Contiene capacidades de IA/ML completamente integradas, gobernanza y gestión de datos, narración visual y colaboración.\n",
    "    - Extraído de: <https://www.tableau.com/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac340a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Google Looker conecta, analiza, y visualiza datos en ambientes multicloud. \n",
    "- Looker puede facilitar la creación de una plataforma de exploración de datos que facilite el acceso a datos de una manera significativa e intuitiva para la organización.\n",
    "- Fuente: <https://looker.com/google-cloud>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfeb6df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Microsoft cuenta por su parte con PowerBI.\n",
    "- PowerBI se conecta a las fuentes de datos, los modela y presenta en paneles con facilidad.\n",
    "- Permite obtener respuestas rápidas y con tecnología de IA a preguntas empresariales.\n",
    "    - Fuente: <https://powerbi.microsoft.com/es-es/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf445b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoreo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a75a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- El seguimiento continuo de los datos es una parte importante de los mecanismos de gobernanza.\n",
    "- Apache Flume es útil para procesar datos de registro.\n",
    "- Apache Storm se utiliza para el monitoreo de operaciones\n",
    "- Apache Spark sirve para transmisión de datos, procesamiento de gráficos y aprendizaje automático.\n",
    "- El monitoreo puede ocurrir en el paso de almacenamiento de datos."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
